{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mail = pd.read_csv('EDA3_1.csv')\n",
    "\n",
    "mail.dropna(axis=0, inplace=True)\n",
    "# mail = mail.applymap(str)\n",
    "mail = mail.replace('spam', 1)\n",
    "mail = mail.replace('ham', 0)\n",
    "mail['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60264 entries, 0 to 60263\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    60264 non-null  object\n",
      " 1   label   60264 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 941.8+ KB\n"
     ]
    }
   ],
   "source": [
    "mail.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = mail['text'].to_list()\n",
    "y = mail['label'].to_list()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2022, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "HUGGINGFACE_MODEL_PATH = \"klue/bert-base\"\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(HUGGINGFACE_MODEL_PATH)\n",
    "\n",
    "# Tokenizing\n",
    "train_encodings = tokenizer(x_train, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(x_test, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset-set\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train\n",
    "))\n",
    "\n",
    "# validation-set\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    y_test\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "num_labels = 2\n",
    "model = TFBertForSequenceClassification.from_pretrained(HUGGINGFACE_MODEL_PATH, num_labels=num_labels, from_pt=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3014/3014 [==============================] - 292s 93ms/step - loss: 0.1057 - accuracy: 0.9604 - val_loss: 0.0386 - val_accuracy: 0.9876\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor=\"val_accuracy\", \n",
    "    min_delta=0.001, # the threshold that triggers the termination (acc should at least improve 0.001)\n",
    "    patience=2)\n",
    "\n",
    "tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = 'ckpt', \n",
    "    monitor='val_loss', \n",
    "    verbose=1, \n",
    "    save_best_only=False,\n",
    "    save_weights_only=False, \n",
    "    mode='auto', \n",
    "    save_freq='epoch')\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    model.fit(\n",
    "        train_dataset.shuffle(50000).batch(16), epochs=1, batch_size=16,\n",
    "        validation_data=val_dataset.shuffle(50000).batch(16),\n",
    "        callbacks = [es]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = pd.read_csv(\"spaced_spam_test_text.csv\", encoding = 'utf-8')\n",
    "test_label = pd.read_csv(\"spam_test_label.csv\", encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "text_classifier = TextClassificationPipeline(\n",
    "    tokenizer=tokenizer, \n",
    "    model=model, \n",
    "    framework='tf',\n",
    "    return_all_scores=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9896/9896 [27:04<00:00,  6.09it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted_label_list = []\n",
    "predicted_score_list = []\n",
    "\n",
    "for text in tqdm(test_text['text']):\n",
    "    # predict\n",
    "    preds_list = text_classifier(text)[0]\n",
    "    sorted_preds_list = sorted(preds_list, key=lambda x: x['score'], reverse=True)\n",
    "    predicted_label_list.append(sorted_preds_list[0]['label']) # label\n",
    "    predicted_score_list.append(sorted_preds_list[0]['score']) # score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_label_list = predicted_label_list.replace('LABEL_1', 1)\n",
    "# predicted_label_list= predicted_label_list.replace('LABEL_0', 0)\n",
    "\n",
    "pred = []\n",
    "f1_pred = []\n",
    "for i in predicted_label_list:\n",
    "    if i == 'LABEL_1':\n",
    "        pred.append('spam')\n",
    "        f1_pred.append(1)\n",
    "    elif i == 'LABEL_0':\n",
    "        pred.append('ham')\n",
    "        f1_pred.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.965931  0.937652  0.951582      3689\n",
      "        spam   0.963579  0.980345  0.971889      6207\n",
      "\n",
      "    accuracy                       0.964430      9896\n",
      "   macro avg   0.964755  0.958999  0.961736      9896\n",
      "weighted avg   0.964456  0.964430  0.964319      9896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "print(classification_report(pred, test_label['label'], digits = 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = test_label['label'].replace(['spam','ham'],[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9718894745248363\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(f1_pred, answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'id':range(0,len(pred)),'label':pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('late_submission_bert.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  110617344 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110,618,882\n",
      "Trainable params: 110,618,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
